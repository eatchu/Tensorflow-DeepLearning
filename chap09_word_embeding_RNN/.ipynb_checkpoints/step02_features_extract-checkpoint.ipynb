{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 텍스트에서 특징(features) 추출 방법\n",
    "   - 지도학습에서 text대상 희소행렬(sparse matrix)\n",
    "   - 방법 : tf, tfidf\n",
    "   \n",
    "2. num_words(max features)\n",
    "   - 단어의 차수를 지정하는 속성\n",
    "   ex) num_words = 500 : 전체 단어(1,000)에서 중요단어 500개 선정\n",
    "\n",
    "3. max length : padding 적용\n",
    "   - 한 문장을 구성하는 단어 수 지정 속성\n",
    "   ex) max_lenght = 100 : 전체 문장을 구성하는 단어수 100개 지정\n",
    "       1문장 : 150개 단어 구성 -> 50개 짤림\n",
    "       2문장 : 70개 단어 구성 -> 30개 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'dog': 2, 'sat': 3, 'on': 4, 'table': 5, 'is': 6, 'my': 7, 'poodle': 8}\n",
      "[[0. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 0.]]\n",
      "[[0. 2. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 0.]]\n",
      "[[0.         0.33333333 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.5        0.5        0.         0.         0.        ]]\n",
      "[[0.         0.86490296 0.51082562 0.69314718 0.69314718 0.69314718]\n",
      " [0.         0.51082562 0.51082562 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # token 생성\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # padding\n",
    "from tensorflow.keras.utils import to_categorical # one-hot-encoding\n",
    "\n",
    "\n",
    "# 토큰 생성기\n",
    "tokenizer = Tokenizer(num_words=6) # 비중 높은 단어 5개 선정 + 1\n",
    "\n",
    "texts = ['The dog sat on the table,', 'The dog is my poodle']\n",
    "\n",
    "# 1. 토큰 생성\n",
    "tokenizer.fit_on_texts(texts) # 텍스트 적용\n",
    "token = tokenbbbizer.word_index # 토큰 반환 : 정수 인덱스 생성\n",
    "print(token) # {word : int index} : 중복 x\n",
    "# {'the': 1, 'dog': 2, 'sat': 3, 'on': 4, 'table': 5, 'is': 6, 'my': 7, 'poodle': 8}\n",
    "\n",
    "\n",
    "# 2. texts -> 특징(features) 추출 : 희소행렬(sparse matrix)\n",
    "binary = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "print(binary) # [docs, terem+1]\n",
    "\n",
    "# 출현빈도수\n",
    "count = tokenizer.texts_to_matrix(texts, mode='count')\n",
    "print(count)\n",
    "\n",
    "# 출현빈도 -> 비율\n",
    "freq = tokenizer.texts_to_matrix(texts, mode='freq')\n",
    "print(freq)\n",
    "\n",
    "# 출현빈도 -> 비율(TF*1/DF)\n",
    "tfidf = tokenizer.texts_to_matrix(texts, mode='tfidf')\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 2]]\n",
      "[6, 2]\n",
      "[[1 2 3 4 1 5]\n",
      " [0 0 0 0 1 2]]\n",
      "[[3 4 1 5]\n",
      " [0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. max length : padding 적용\n",
    "seq_index = tokenizer.texts_to_sequences(texts)\n",
    "print(seq_index) # [[1, 2, 3, 4, 1, 5], [1, 2]]\n",
    "\n",
    "lens = [len(seq) for seq in seq_index]\n",
    "print(lens) # [6,2]\n",
    "\n",
    "max_length = max(lens)\n",
    "max_length # 6\n",
    "\n",
    "x_data1 = pad_sequences(seq_index, maxlen=max_length)\n",
    "print(x_data1)\n",
    "\n",
    "x_data2 = pad_sequences(seq_index, maxlen = 4)\n",
    "print(x_data2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
